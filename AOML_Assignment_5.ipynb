{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fgX15v3m7fiD",
        "outputId": "47f66b5a-e0d2-45e8-b80a-da1b41c4bc1f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OWtRLSsW8u36",
        "outputId": "4266d2f1-0147-453d-fe3f-c87bae26018e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"spam.csv\", encoding='latin-1')\n",
        "df = df[['v1', 'v2']]\n",
        "df.columns = ['label', 'text']\n",
        "df['label'] = df['label'].map({'ham': 0, 'spam': 1})\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(f\"[{string.punctuation}]\", \"\", text)\n",
        "    text = re.sub(r\"\\d+\", \"\", text)\n",
        "    words = nltk.word_tokenize(text)\n",
        "    words = [w for w in words if w not in stopwords.words('english')]\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    words = [lemmatizer.lemmatize(w) for w in words]\n",
        "    return \" \".join(words)\n",
        "\n",
        "df['cleaned_text'] = df['text'].apply(clean_text)\n",
        "\n",
        "vectorizers = {\n",
        "    \"bow\": CountVectorizer(),\n",
        "    \"tfidf\": TfidfVectorizer()\n",
        "}"
      ],
      "metadata": {
        "id": "N5P5V9L97j4I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for name, vectorizer in vectorizers.items():\n",
        "    X = vectorizer.fit_transform(df['text'])\n",
        "    X_cleaned = vectorizer.fit_transform(df['cleaned_text'])\n",
        "    y = df['label']\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "    X_train_cleaned, X_test_cleaned, _, _ = train_test_split(X_cleaned, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    models = {\n",
        "        \"Naive Bayes\": MultinomialNB(),\n",
        "        \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
        "        \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
        "    }\n",
        "\n",
        "    for model_name, model in models.items():\n",
        "        for data, X_tr, X_te in [(\"Original\", X_train, X_test), (\"Cleaned\", X_train_cleaned, X_test_cleaned)]:\n",
        "            model.fit(X_tr, y_train)\n",
        "            y_pred = model.predict(X_te)\n",
        "            acc = accuracy_score(y_test, y_pred)\n",
        "            print(f\"{model_name} - {name.upper()} ({data}): {acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "05TnUN3P7z36",
        "outputId": "198bd290-d2bc-496a-d49d-b8083aed3607"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Naive Bayes - BOW (Original): 0.9785\n",
            "Naive Bayes - BOW (Cleaned): 0.9686\n",
            "Random Forest - BOW (Original): 0.9749\n",
            "Random Forest - BOW (Cleaned): 0.9731\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [18:24:26] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost - BOW (Original): 0.9776\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [18:24:26] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost - BOW (Cleaned): 0.9722\n",
            "Naive Bayes - TFIDF (Original): 0.9623\n",
            "Naive Bayes - TFIDF (Cleaned): 0.9650\n",
            "Random Forest - TFIDF (Original): 0.9767\n",
            "Random Forest - TFIDF (Cleaned): 0.9704\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [18:24:37] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost - TFIDF (Original): 0.9821\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [18:24:38] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost - TFIDF (Cleaned): 0.9740\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ensemble = VotingClassifier(\n",
        "    estimators=[\n",
        "        ('nb', MultinomialNB()),\n",
        "        ('rf', RandomForestClassifier(n_estimators=100, random_state=42)),\n",
        "        ('xgb', XGBClassifier(use_label_encoder=False, eval_metric='logloss'))\n",
        "    ], voting='soft')\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_cleaned, y, test_size=0.2, random_state=42)\n",
        "ensemble.fit(X_train, y_train)\n",
        "y_pred_ensemble = ensemble.predict(X_test)\n",
        "ensemble_acc = accuracy_score(y_test, y_pred_ensemble)\n",
        "print(f\"Ensemble Model Accuracy: {ensemble_acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m5cJsVV773Zx",
        "outputId": "00264088-42af-4b36-edf2-aa6a61c8b884"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [18:24:53] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ensemble Model Accuracy: 0.9740\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QfdojNWQ8_kY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}